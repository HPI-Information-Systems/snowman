{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Snowman Have you ever gotten a newsletter twice? Probably - finding duplicates in data is a pretty difficult problem. Many different matching solutions exist with different approaches and tradeoffs. However, not only finding duplicates is difficult but also finding a deduplication solution for a dataset or even just comparing two different deduplication solutions. With our benchmark, we aim to solve exactly this challenge. Snowman is developed as part of a bachelor's project in collaboration with SAP SE. For a quick first impression, jump directly into the analyses overview . Usage If you want to use our benchmark, please consult the section Basic usage for details. You can also use Snowman from your code or create a shared instance of Snowman . Contributing If you intend to contribute to our project, please take a look at the Development section . It includes information on how to get started. Licenses Copyright 2021 Hasso Plattner Institute. Licensed under the MIT license. A complete list of all dependencies and their individual licenses can be found here: Licenses JSON","title":"Home"},{"location":"#welcome-to-snowman","text":"Have you ever gotten a newsletter twice? Probably - finding duplicates in data is a pretty difficult problem. Many different matching solutions exist with different approaches and tradeoffs. However, not only finding duplicates is difficult but also finding a deduplication solution for a dataset or even just comparing two different deduplication solutions. With our benchmark, we aim to solve exactly this challenge. Snowman is developed as part of a bachelor's project in collaboration with SAP SE. For a quick first impression, jump directly into the analyses overview .","title":"Welcome to Snowman"},{"location":"#usage","text":"If you want to use our benchmark, please consult the section Basic usage for details. You can also use Snowman from your code or create a shared instance of Snowman .","title":"Usage"},{"location":"#contributing","text":"If you intend to contribute to our project, please take a look at the Development section . It includes information on how to get started.","title":"Contributing"},{"location":"#licenses","text":"Copyright 2021 Hasso Plattner Institute. Licensed under the MIT license. A complete list of all dependencies and their individual licenses can be found here: Licenses JSON","title":"Licenses"},{"location":"sigmod2021/","text":"ACM SIGMOD 2021 Welcome participants of the ACM SIGMOD 2021 programming contest ! This page contains some special information for you to get started more easily with our tool. Feel free to open an issue in case you find a bug or see space for improvement :) Introduction This tool will help you to compare and evaluate your data matching solutions. You can upload experiment results from your data matching solution and then compare it with a goldstandard, compare two experiment runs with each other or calculate binary metrics like precision or recall. We are working closely with the contest team to allow for a seamless experience. Setup To use snowman, you'll have to download the latest release (Github Releases) of snowman for your specific os. After that you are able to start snowman! As a first step, we'd suggest for you to start with our section Basic Usage or take a look at our intoductory video below for a guide on how to benchmark and evaluate matching solutions with snowman. You can find an explanation of a basic workflow here or in the video below. Some contest datasets are already bundled with the release. Select one of them, upload your own experiment result and start benchmarking! In case you'll want to upgrade later on, simply download the newest release and you're ready to go! Datasets We've included an automatic importer for the gold standard format and validated that the below datasets can be imported out of the box. More datasets will be added as soon as they are released by the contest team. NotebookToy : prepackaged dataset X1 and gold standard Y1 Notebook : prepackaged dataset X2 and gold standard Y2 NotebookLarge : prepackaged dataset X3 and gold standard Y3 AltoSight : prepackaged dataset X4 and gold standard Y4 If you want to upload further datasets which have the same format as the SIGMOD-datasets, you will have to change some default settings in the dataset-uploader-dialog: Set the ID Column from id to instance_id and set the Escape character to \" . You can then select the dataset file and click on ADD . Appendix We wish all participants best of luck!","title":"SIGMOD2021"},{"location":"sigmod2021/#acm-sigmod-2021","text":"Welcome participants of the ACM SIGMOD 2021 programming contest ! This page contains some special information for you to get started more easily with our tool. Feel free to open an issue in case you find a bug or see space for improvement :)","title":"ACM SIGMOD 2021"},{"location":"sigmod2021/#introduction","text":"This tool will help you to compare and evaluate your data matching solutions. You can upload experiment results from your data matching solution and then compare it with a goldstandard, compare two experiment runs with each other or calculate binary metrics like precision or recall. We are working closely with the contest team to allow for a seamless experience.","title":"Introduction"},{"location":"sigmod2021/#setup","text":"To use snowman, you'll have to download the latest release (Github Releases) of snowman for your specific os. After that you are able to start snowman! As a first step, we'd suggest for you to start with our section Basic Usage or take a look at our intoductory video below for a guide on how to benchmark and evaluate matching solutions with snowman. You can find an explanation of a basic workflow here or in the video below. Some contest datasets are already bundled with the release. Select one of them, upload your own experiment result and start benchmarking! In case you'll want to upgrade later on, simply download the newest release and you're ready to go!","title":"Setup"},{"location":"sigmod2021/#datasets","text":"We've included an automatic importer for the gold standard format and validated that the below datasets can be imported out of the box. More datasets will be added as soon as they are released by the contest team. NotebookToy : prepackaged dataset X1 and gold standard Y1 Notebook : prepackaged dataset X2 and gold standard Y2 NotebookLarge : prepackaged dataset X3 and gold standard Y3 AltoSight : prepackaged dataset X4 and gold standard Y4 If you want to upload further datasets which have the same format as the SIGMOD-datasets, you will have to change some default settings in the dataset-uploader-dialog: Set the ID Column from id to instance_id and set the Escape character to \" . You can then select the dataset file and click on ADD .","title":"Datasets"},{"location":"sigmod2021/#appendix","text":"We wish all participants best of luck!","title":"Appendix"},{"location":"advanced_setups/code_connection/","text":"Connect your Code To more easily pipe results from your code into Snowman, you can make use of its REST API . The following guide will outline the necessary steps and give you a brief introduction. Snowman API Whenever a local Snowman instance is running, the API is listening at http://localhost:8123/api (can be configured via command line arguments ). This interface is also used by the Snowman frontend. Warning Be advised: We have not yet implemented any security features or authorization. That means every process with access to the host is able to access the API. Progress is tracked in issue #107 . You can upload results in two steps: Create a new experiment. Upload data to it. Use Case In this example, we want to export our results from a python script running a ML matching solution. Consider the following code: import io import csv ... # Results produced by the ML solution candidate_pairs = ... # Create a new string builder to write to output = io.StringIO() writer = csv.writer(output, quoting=csv.QUOTE_NONNUMERIC) # Write the CSV header writer.writerow(['p1', 'p2', 'prediction']) for candidate_pair in candidate_pairs: # Write each row, prediction/label is either 0 (no duplicate) or 1 (duplicate) writer.writerow([candidate_pair['id1'], candidate_pair['id2'], candidate_pair['label']]) # CSV string to upload to Snowman csv_string = output.getvalue() print(csv_string) With this code, we are able to convert the results into a csv format the API is able to understand - in this case, the pilot format. It remains to upload the csv data. Upload Results As outlined above, this procedure consists of two steps. First, create a new experiment. Note: You will need a datasetId and algorithmId to assign the experiment to. We recommend creating (or selecting) a dataset and matching solution (algorithm) with the UI. The IDs are shown in brackets behind the header of all edit dialogs. import requests new_experiment_payload = {'datasetId': 5, 'algorithmId': 2, 'name':'my-example-run-01','description':'automatic-upload'} create_experiment_response = requests.post('http://localhost:8123/api/v1/experiments', json=new_experiment_payload) new_experiment_id = create_experiment_response.text print(new_experiment_id) This will return the new experiment's id which you will need in the next step. Now upload the data we already exported to csv: import requests upload_pairs_response = requests.put(f'http://localhost:8123/api/v1/experiments/{new_experiment_id}/file?format=pilot', data=csv_string, headers={'Content-Type': 'text/csv'}) print(upload_pairs_response.status_code) If both request return a 200er status code, the upload process is complete.","title":"Connect your Code"},{"location":"advanced_setups/code_connection/#connect-your-code","text":"To more easily pipe results from your code into Snowman, you can make use of its REST API . The following guide will outline the necessary steps and give you a brief introduction.","title":"Connect your Code"},{"location":"advanced_setups/code_connection/#snowman-api","text":"Whenever a local Snowman instance is running, the API is listening at http://localhost:8123/api (can be configured via command line arguments ). This interface is also used by the Snowman frontend. Warning Be advised: We have not yet implemented any security features or authorization. That means every process with access to the host is able to access the API. Progress is tracked in issue #107 . You can upload results in two steps: Create a new experiment. Upload data to it.","title":"Snowman API"},{"location":"advanced_setups/code_connection/#use-case","text":"In this example, we want to export our results from a python script running a ML matching solution. Consider the following code: import io import csv ... # Results produced by the ML solution candidate_pairs = ... # Create a new string builder to write to output = io.StringIO() writer = csv.writer(output, quoting=csv.QUOTE_NONNUMERIC) # Write the CSV header writer.writerow(['p1', 'p2', 'prediction']) for candidate_pair in candidate_pairs: # Write each row, prediction/label is either 0 (no duplicate) or 1 (duplicate) writer.writerow([candidate_pair['id1'], candidate_pair['id2'], candidate_pair['label']]) # CSV string to upload to Snowman csv_string = output.getvalue() print(csv_string) With this code, we are able to convert the results into a csv format the API is able to understand - in this case, the pilot format. It remains to upload the csv data.","title":"Use Case"},{"location":"advanced_setups/code_connection/#upload-results","text":"As outlined above, this procedure consists of two steps. First, create a new experiment. Note: You will need a datasetId and algorithmId to assign the experiment to. We recommend creating (or selecting) a dataset and matching solution (algorithm) with the UI. The IDs are shown in brackets behind the header of all edit dialogs. import requests new_experiment_payload = {'datasetId': 5, 'algorithmId': 2, 'name':'my-example-run-01','description':'automatic-upload'} create_experiment_response = requests.post('http://localhost:8123/api/v1/experiments', json=new_experiment_payload) new_experiment_id = create_experiment_response.text print(new_experiment_id) This will return the new experiment's id which you will need in the next step. Now upload the data we already exported to csv: import requests upload_pairs_response = requests.put(f'http://localhost:8123/api/v1/experiments/{new_experiment_id}/file?format=pilot', data=csv_string, headers={'Content-Type': 'text/csv'}) print(upload_pairs_response.status_code) If both request return a 200er status code, the upload process is complete.","title":"Upload Results"},{"location":"advanced_setups/remote_instances/","text":"Remote Instances A Snowman instance can be hosted to allow remote access. This can be useful in case you want to collaborate with coworkers or provide an instance for a client to connect to. We refer to such instances as remote instances . Warning Be advised: We have not yet implemented any security features or authorization. That means every process with access to the host is able to access the API. Progress is tracked in issue #107 . We therefore recommend to all users to set up authentication and encryption for their remote instances by themselfs. This can be achieved through reverse proxies like Caddy or nginx . How it Works Snowman is a web server and web app packaged with Electron. That means accessing a remote instance is as easy as visiting the website of the remote instance (this can be done with the Snowman client or a normal web browser). By using this setup we avoid version conflicts as a client will always use the frontend provided by the remote instance. Client Perspective As a client, you will receive the URL (=address) of the Snowman instance you should connect to. When opening Snowman app, make sure to not start a local instance but selecting REMOTE INSTANCE and entering the provided address. Provider Perspective As a provider, you have two options for hosting a Snowman instance. Headless Mode Snowman offers a headless mode (does not open a GUI window). Access it by including the --headless flag when starting snowman from the command line. For further command line arguments and how to configure host and port see here . Example: ./snowman --headless --hostname 192.168.12.34 --port 8123 --storageDirectory /home/user/snowman In this case, the address http://192.168.12.34:8123 can be used by clients to connect to this instance. Docker As an alternative, we provide a fully featured Docker image that runs a Snowman server. This feature is WIP - progress is tracked in issue #216 !","title":"Remote Instances"},{"location":"advanced_setups/remote_instances/#remote-instances","text":"A Snowman instance can be hosted to allow remote access. This can be useful in case you want to collaborate with coworkers or provide an instance for a client to connect to. We refer to such instances as remote instances . Warning Be advised: We have not yet implemented any security features or authorization. That means every process with access to the host is able to access the API. Progress is tracked in issue #107 . We therefore recommend to all users to set up authentication and encryption for their remote instances by themselfs. This can be achieved through reverse proxies like Caddy or nginx .","title":"Remote Instances"},{"location":"advanced_setups/remote_instances/#how-it-works","text":"Snowman is a web server and web app packaged with Electron. That means accessing a remote instance is as easy as visiting the website of the remote instance (this can be done with the Snowman client or a normal web browser). By using this setup we avoid version conflicts as a client will always use the frontend provided by the remote instance.","title":"How it Works"},{"location":"advanced_setups/remote_instances/#client-perspective","text":"As a client, you will receive the URL (=address) of the Snowman instance you should connect to. When opening Snowman app, make sure to not start a local instance but selecting REMOTE INSTANCE and entering the provided address.","title":"Client Perspective"},{"location":"advanced_setups/remote_instances/#provider-perspective","text":"As a provider, you have two options for hosting a Snowman instance.","title":"Provider Perspective"},{"location":"advanced_setups/remote_instances/#headless-mode","text":"Snowman offers a headless mode (does not open a GUI window). Access it by including the --headless flag when starting snowman from the command line. For further command line arguments and how to configure host and port see here . Example: ./snowman --headless --hostname 192.168.12.34 --port 8123 --storageDirectory /home/user/snowman In this case, the address http://192.168.12.34:8123 can be used by clients to connect to this instance.","title":"Headless Mode"},{"location":"advanced_setups/remote_instances/#docker","text":"As an alternative, we provide a fully featured Docker image that runs a Snowman server. This feature is WIP - progress is tracked in issue #216 !","title":"Docker"},{"location":"advanced_setups/restish-cli/","text":"Rest!sh CLI The restish project is a general purpose api client for the command line. To make accessing the data easier, Snowman's api is compatible with restish's interface. Getting started As a first step, follow restish's getting started guide to install a recent version of the software. Afterwards, start your local snowman instance - by default, its api will then be available at http://localhost:8123 . Run the following command to get a brief overview: restish localhost:8123 --help Now, (almost) every api functionality can be accessed as easily as: restish localhost:8123 <function> -q <parameter> More information See restish docs for more information and complex operations.","title":"Using via CLI"},{"location":"advanced_setups/restish-cli/#restsh-cli","text":"The restish project is a general purpose api client for the command line. To make accessing the data easier, Snowman's api is compatible with restish's interface.","title":"Rest!sh CLI"},{"location":"advanced_setups/restish-cli/#getting-started","text":"As a first step, follow restish's getting started guide to install a recent version of the software. Afterwards, start your local snowman instance - by default, its api will then be available at http://localhost:8123 . Run the following command to get a brief overview: restish localhost:8123 --help Now, (almost) every api functionality can be accessed as easily as: restish localhost:8123 <function> -q <parameter>","title":"Getting started"},{"location":"advanced_setups/restish-cli/#more-information","text":"See restish docs for more information and complex operations.","title":"More information"},{"location":"basic_usage/concepts/","text":"Concepts and Terminology At the end of this page, you will understand the terminology and the key concepts used in Snowman. Terminology Entity Types Snowman supports all entity types which are present in matching workflows. Concept Definition Dataset A list of records which can but does not have to contain duplicates. Snowman only supports relational datasets. Matching Solution A Tool to deduplicate datasets. Experiment The list of predicted duplicates produced when deduplicating a dataset with a matching solution. That means that one matching solution can have multiple experiments belonging to the same dataset (e.g. corresponding to different configurations of the matching solution). Example Imagine you are running the Magellan data matching tool (open-source). It allows you to configure several aspects of how it operates and thereby a lot of customization is possible. For this use case, you would only create one matching solution Magellan within the tool. For each configuration, you can afterwards create a new experiment containing information on the configuration within its description. Have a look at the pages Datasets , Matching solutions , and Experiments for information on how to manage them in Snowman. Special Entities A gold standard is an experiment containing all duplicates a dataset contains. A silver standard is an experiment containing a subset of the duplicates a dataset contains. Altough there are different ways to produce gold standards and silver standards, Snowman puts them into one category. Therefore Snowman provides a gold standard matching solution and a silver standard matching solution out of the box. Assigning experiments to these matching solutions will let Snowman know whether an experiment is a gold standard or a silver standard. Similarity Scores Some matching solutions output a similarity score next to the matching decision. This score defines whether a pair is considered duplicate or not. Snowman can make use of this information and allows you to define a similarity score for each experiment on every evaluation page. Afterwards, a similarity threshold can be used to declare all pairs with a similarity score higher than this threshold as duplicate and all others as non duplicates. Next Steps You should now have an understanding of the terminoloy and key concepts used in Snowman. As a next step, have a look at how you can configure Snowman to get the most out of the analyses we provide.","title":"Concepts and Terminology"},{"location":"basic_usage/concepts/#concepts-and-terminology","text":"At the end of this page, you will understand the terminology and the key concepts used in Snowman.","title":"Concepts and Terminology"},{"location":"basic_usage/concepts/#terminology","text":"","title":"Terminology"},{"location":"basic_usage/concepts/#entity-types","text":"Snowman supports all entity types which are present in matching workflows. Concept Definition Dataset A list of records which can but does not have to contain duplicates. Snowman only supports relational datasets. Matching Solution A Tool to deduplicate datasets. Experiment The list of predicted duplicates produced when deduplicating a dataset with a matching solution. That means that one matching solution can have multiple experiments belonging to the same dataset (e.g. corresponding to different configurations of the matching solution).","title":"Entity Types"},{"location":"basic_usage/concepts/#example","text":"Imagine you are running the Magellan data matching tool (open-source). It allows you to configure several aspects of how it operates and thereby a lot of customization is possible. For this use case, you would only create one matching solution Magellan within the tool. For each configuration, you can afterwards create a new experiment containing information on the configuration within its description. Have a look at the pages Datasets , Matching solutions , and Experiments for information on how to manage them in Snowman.","title":"Example"},{"location":"basic_usage/concepts/#special-entities","text":"A gold standard is an experiment containing all duplicates a dataset contains. A silver standard is an experiment containing a subset of the duplicates a dataset contains. Altough there are different ways to produce gold standards and silver standards, Snowman puts them into one category. Therefore Snowman provides a gold standard matching solution and a silver standard matching solution out of the box. Assigning experiments to these matching solutions will let Snowman know whether an experiment is a gold standard or a silver standard.","title":"Special Entities"},{"location":"basic_usage/concepts/#similarity-scores","text":"Some matching solutions output a similarity score next to the matching decision. This score defines whether a pair is considered duplicate or not. Snowman can make use of this information and allows you to define a similarity score for each experiment on every evaluation page. Afterwards, a similarity threshold can be used to declare all pairs with a similarity score higher than this threshold as duplicate and all others as non duplicates.","title":"Similarity Scores"},{"location":"basic_usage/concepts/#next-steps","text":"You should now have an understanding of the terminoloy and key concepts used in Snowman. As a next step, have a look at how you can configure Snowman to get the most out of the analyses we provide.","title":"Next Steps"},{"location":"basic_usage/configuring_analyses/","text":"Configuring Analyses After reading this page you will know how to configure Snowman for the different analyses. Benchmark Dashboard Open the Benchmark tab. You can select the different available analyses by clicking on Start Benchmark of the analyses cards. Info In many places you can get additional information about objects on the screen by hovering over them. Try hovering over Data Stewards or Developers on the benchmark dashboard. Configurator After opening an analysis, the subset of experiments, datasets, or matching solutions to be analysed must be selected. For this we provide a configurator specific to every analysis in the sidebar on the left. If the configuration for the analysis is not complete, a warning will be shown containing information on how to complete the configuration. Info Pro Tip: Every entity can be opened and edited by clicking on it's icon. Next Step Now you should know how to configure Snowman for the different analyses. As a next step we suggest that you take a look at the different analyses you can perform with snowman to compare and find the best matching solution or to improve your matching solution If you are unsure, have a look at all analyses .","title":"Configuring Analyses"},{"location":"basic_usage/configuring_analyses/#configuring-analyses","text":"After reading this page you will know how to configure Snowman for the different analyses.","title":"Configuring Analyses"},{"location":"basic_usage/configuring_analyses/#benchmark-dashboard","text":"Open the Benchmark tab. You can select the different available analyses by clicking on Start Benchmark of the analyses cards. Info In many places you can get additional information about objects on the screen by hovering over them. Try hovering over Data Stewards or Developers on the benchmark dashboard.","title":"Benchmark Dashboard"},{"location":"basic_usage/configuring_analyses/#configurator","text":"After opening an analysis, the subset of experiments, datasets, or matching solutions to be analysed must be selected. For this we provide a configurator specific to every analysis in the sidebar on the left. If the configuration for the analysis is not complete, a warning will be shown containing information on how to complete the configuration. Info Pro Tip: Every entity can be opened and edited by clicking on it's icon.","title":"Configurator"},{"location":"basic_usage/configuring_analyses/#next-step","text":"Now you should know how to configure Snowman for the different analyses. As a next step we suggest that you take a look at the different analyses you can perform with snowman to compare and find the best matching solution or to improve your matching solution If you are unsure, have a look at all analyses .","title":"Next Step"},{"location":"basic_usage/datasets/","text":"Datasets At the end of this tutorial you will know why and how to add your datasets to Snowman. Additionally you will know what you can do with datasets once they are in the tool. Adding a dataset To add and analyse an experiment inside Snowman, you first have to add the dataset which should be deduplicated. Open the Datasets tab. Click on the + button in the lower left corner of the screen. Give the dataset a short name and optionally a comprehensive description. You can also add tags to the dataset at the bottom of the dialog. Select a CSV file containing the dataset and specify the CSV parameters. The ID column contains a unique alphanumeric identifier for every record. If you do not have access to the dataset records, select Record count only as contents and specify the total amount of records in the dataset. Click on Add to add the dataset to Snowman. this process can take several minutes depending on the size of the dataset. You should now see your dataset in the Datasets tab Previewing a dataset You can preview the dataset by clicking on the telescope button on the bottom of the dataset card to make sure that the dataset was added successfully. Info Pro Tip: Click on the button in the top right corner of any table (which appears when the mouse is over the table) to open the table in a new window. Editing a dataset After the initial dataset creation, you can still change some attributes of the dataset. To open the dataset editor, click the leftmost button on the dataset card. Deleting a dataset To delete a dataset click the rightmost button on the dataset card. Be aware that all experiments belonging to this dataset will automatically be deleted.","title":"Datasets"},{"location":"basic_usage/datasets/#datasets","text":"At the end of this tutorial you will know why and how to add your datasets to Snowman. Additionally you will know what you can do with datasets once they are in the tool.","title":"Datasets"},{"location":"basic_usage/datasets/#adding-a-dataset","text":"To add and analyse an experiment inside Snowman, you first have to add the dataset which should be deduplicated. Open the Datasets tab. Click on the + button in the lower left corner of the screen. Give the dataset a short name and optionally a comprehensive description. You can also add tags to the dataset at the bottom of the dialog. Select a CSV file containing the dataset and specify the CSV parameters. The ID column contains a unique alphanumeric identifier for every record. If you do not have access to the dataset records, select Record count only as contents and specify the total amount of records in the dataset. Click on Add to add the dataset to Snowman. this process can take several minutes depending on the size of the dataset. You should now see your dataset in the Datasets tab","title":"Adding a dataset"},{"location":"basic_usage/datasets/#previewing-a-dataset","text":"You can preview the dataset by clicking on the telescope button on the bottom of the dataset card to make sure that the dataset was added successfully. Info Pro Tip: Click on the button in the top right corner of any table (which appears when the mouse is over the table) to open the table in a new window.","title":"Previewing a dataset"},{"location":"basic_usage/datasets/#editing-a-dataset","text":"After the initial dataset creation, you can still change some attributes of the dataset. To open the dataset editor, click the leftmost button on the dataset card.","title":"Editing a dataset"},{"location":"basic_usage/datasets/#deleting-a-dataset","text":"To delete a dataset click the rightmost button on the dataset card. Be aware that all experiments belonging to this dataset will automatically be deleted.","title":"Deleting a dataset"},{"location":"basic_usage/experiments/","text":"Experiments At the end of this page you will know how to add experiments to Snowman. Additionally you will know what you can do with experiments once they are in the tool. Adding an experiment Open the Experiments tab. Click on the + button in the lower left corner of the screen. Specify a short name and optionally a comprehensive description. Choose the dataset which this experiment deduplicates and the matching solution which created this experiment. Optionally, open the Configuration Effort and Other KPIs sections and fill in details. Matching Solution Knowledge Level measures how much you know about the matching solution. Matching Solution HR Amount measures how much time you spent configuring the matching solution to produce this experiment. Select a file containing the output of the matching solution and choose the correct import format . Click on Add . this process can take several minutes depending on the size of the experiment. Previewing an experiment You can preview the experiment by clicking on the telescope button on the bottom of the dataset card to make sure that the experiment was added successfully. Editing an experiment After the initial experiment creation, you can still change some attributes of the dataset. To open the experiment editor, click the leftmost button on the experiment card. Deleting an experiment To delete an experiment click the rightmost button on the experiment card. Import formats Info The tool only accepts source files in csv format at the moment - so in case your source file is a Microsoft Excel file, you'll first have to export it to csv! (Additionally, make sure that your file is UTF-8 encoded.) To ease the import process, Snowman supports several file formats out of the box. Those include: Pilot This file format was introduced with the initial prototype and is the easiest available format. Result sets can be uploaded as (comma-separated) csv files in UTF-8 encoding. We expect the csv file to have , as split character, \" as quote character and ' as escape character. The importer expects the csv to have the columns p1 and p2 . They should store the ids of the respective dataset tuples. The csv optionally can have a column named prediction which contains a 1 in case the pair was detected as duplicate - 0 otherwise. If this column is not present we assume that all listed tuples have been detected as duplicates (think: we automatically insert a column prediction and fill it with the value 1 everywhere). Following this, more columns may be specified which contain similarity scores (numbers). See the following example: p1,p2,prediction,feat1,feat2,feat3,sum 2,1,1,0.3,0.4,0.4,2 1,2,1,0.3,0.4,0.4,2 4,3,1,0.3,0.4,0.4,2 3,4,1,0.3,0.4,0.4,3 6,5,1,0.3,0.5,0.4,3 5,6,1,0.3,0.5,0.4,3 8,7,1,0.3,0.5,0.4,2 7,8,1,0.3,0.5,0.4,2 10,9,1,0.3,0.5,0.4,2 9,10,1,0.3,0.4,0.4,2 12,11,1,0.3,0.4,0.4,2 11,12,1,0.3,0.4,0.4,8 14,13,1,0.3,0.4,0.4,8 13,14,1,0.3,0.4,0.4,8 16,15,1,0.3,0.4,0.4,8 15,16,1,0.3,0.4,0.4,2 ... Magellan The open-source matching solution Magellan is widely used in research. We support its result set file format out of the box. ,_id,ltable_id,rtable_id,id_id_exm,id_id_anm,id_id_lev_dist,id_id_lev_sim,name_name_jac_qgm_3_qgm_3,name_name_cos_dlm_dc0_dlm_dc0,name_name_jac_dlm_dc0_dlm_dc0,name_name_mel,name_name_lev_dist,name_name_lev_sim,name_name_nmw,name_name_sw,addr_addr_jac_qgm_3_qgm_3,addr_addr_cos_dlm_dc0_dlm_dc0,addr_addr_jac_dlm_dc0_dlm_dc0,addr_addr_mel,addr_addr_lev_dist,addr_addr_lev_sim,addr_addr_nmw,addr_addr_sw,city_city_jac_qgm_3_qgm_3,city_city_cos_dlm_dc0_dlm_dc0,city_city_jac_dlm_dc0_dlm_dc0,city_city_mel,city_city_lev_dist,city_city_lev_sim,city_city_nmw,city_city_sw,type_type_jac_qgm_3_qgm_3,type_type_cos_dlm_dc0_dlm_dc0,type_type_jac_dlm_dc0_dlm_dc0,type_type_mel,type_type_lev_dist,type_type_lev_sim,type_type_nmw,type_type_sw,gold,predicted 124,2054,598,283,0,0.47324414715719065,3,0.0,1.0,1.0,1.0,1.0,0.0,1.0,11.0,11.0,0.21739130434782608,0.40824829046386296,0.2222222222222222,0.7647619247436523,30.0,0.25,-16.0,9.0,0.47058823529411764,0.8164965809277259,0.6666666666666666,0.9230769276618958,5.0,0.6153846153846154,3.0,8.0,0.0,0.0,0.0,0.4991452991962433,11.0,0.15384615384615385,-6.0,1.0,1,1 54,768,739,115,0,0.15561569688768606,3,0.0,0.1111111111111111,0.40824829046386296,0.25,0.5292929410934448,12.0,0.19999999999999996,-1.0,4.0,0.14814814814814814,0.2886751345948129,0.16666666666666666,0.5842490792274475,9.0,0.3571428571428571,4.0,4.0,0.47058823529411764,0.8164965809277259,0.6666666666666666,0.9230769276618958,5.0,0.6153846153846154,3.0,8.0,0.0,0.0,0.0,0.0,6.0,0.0,-2.0,1.0,0,0 268,1736,1046,246,0,0.23518164435946465,2,0.5,0.058823529411764705,0.33333333333333337,0.2,0.5309057235717773,15.0,0.11764705882352944,0.0,4.0,0.06896551724137931,0.0,0.0,0.6170329451560974,10.0,0.2857142857142857,3.0,4.0,0.0,0.0,0.0,0.5897436141967773,10.0,0.23076923076923073,2.0,3.0,0.3,0.7071067811865475,0.5,0.875,10.0,0.375,-4.0,6.0,0,0 293,618,888,78,0,0.08783783783783783,2,0.33333333333333337,0.2,0.4999999999999999,0.3333333333333333,0.6702020168304443,9.0,0.4,2.0,5.0,0.10810810810810811,0.22360679774997896,0.125,0.510185182094574,17.0,0.29166666666666663,-2.0,6.0,0.47058823529411764,0.8164965809277259,0.6666666666666666,0.9230769276618958,5.0,0.6153846153846154,3.0,8.0,0.0,0.0,0.0,0.5777778029441833,6.0,0.0,-1.0,1.0,0,0 230,486,866,66,0,0.07621247113163976,1,0.6666666666666667,0.18181818181818182,0.35355339059327373,0.2,0.35333332419395447,18.0,0.28,-8.0,6.0,0.02857142857142857,0.0,0.0,0.4783068895339966,14.0,0.2222222222222222,0.0,2.0,0.0,0.0,0.0,0.4611110985279083,13.0,0.1333333333333333,-5.0,1.0,0.0,0.0,0.0,0.5138888955116272,7.0,0.125,-2.0,2.0,0,0 134,2079,599,284,0,0.4741235392320534,3,0.0,1.0,1.0,1.0,1.0,0.0,1.0,17.0,17.0,1.0,1.0,1.0,1.0,0.0,1.0,14.0,14.0,0.47058823529411764,0.8164965809277259,0.6666666666666666,0.9230769276618958,5.0,0.6153846153846154,3.0,8.0,0.4444444444444444,0.7071067811865475,0.5,0.9142857193946838,6.0,0.5714285714285714,2.0,8.0,1,1 12,391,905,48,0,0.053038674033149213,3,0.0,0.19230769230769232,0.40824829046386296,0.25,0.6499999761581421,9.0,0.4,3.0,6.0,0.023809523809523808,0.0,0.0,0.47883597016334534,18.0,0.1428571428571429,0.0,2.0,0.0,0.0,0.0,0.5416666865348816,9.0,0.25,-1.0,3.0,0.4444444444444444,0.7071067811865475,0.5,0.7321428656578064,6.0,0.5714285714285714,2.0,8.0,0,0 423,1450,758,209,0,0.2757255936675461,3,0.0,0.21875,0.35355339059327373,0.2,0.5888888835906982,12.0,0.4,3.0,7.0,0.07692307692307693,0.0,0.0,0.5629629492759705,11.0,0.2666666666666667,-2.0,4.0,0.0,0.0,0.0,0.5352563858032227,12.0,0.07692307692307687,-4.0,2.0,0.0,0.0,0.0,0.4833333194255829,8.0,0.19999999999999996,0.0,2.0,0,0 272,248,797,33,0,0.04140526976160597,3,0.0,0.20833333333333334,0.40824829046386296,0.25,0.6168830990791321,8.0,0.4285714285714286,3.0,6.0,0.2,0.5,0.3333333333333333,0.7516340017318726,9.0,0.47058823529411764,6.0,7.0,0.0,0.0,0.0,0.0,8.0,0.0,-6.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,8.0,8.0,0,0 ... SIGMOD2021 For more information, see here . left_instance_id,right_instance_id,label http://store.com/42,http://otherstore.net/af82,1 http://store.com/243,http://otherstore.net/cn82,0 ... The label (1=duplicate, 0=non-duplicate) is optional in Snowman. A missing label will consider the pair as duplicates. Therefore, the following list would be interpreted as duplicates: left_instance_id,right_instance_id http://store.com/42,http://otherstore.net/af82 http://store.com/243,http://otherstore.net/cn82 ... Proprietary Formats We support a range of proprietary experiment formats. A list of those can be found here: BHANA BPIES ClusterER","title":"Experiments"},{"location":"basic_usage/experiments/#experiments","text":"At the end of this page you will know how to add experiments to Snowman. Additionally you will know what you can do with experiments once they are in the tool.","title":"Experiments"},{"location":"basic_usage/experiments/#adding-an-experiment","text":"Open the Experiments tab. Click on the + button in the lower left corner of the screen. Specify a short name and optionally a comprehensive description. Choose the dataset which this experiment deduplicates and the matching solution which created this experiment. Optionally, open the Configuration Effort and Other KPIs sections and fill in details. Matching Solution Knowledge Level measures how much you know about the matching solution. Matching Solution HR Amount measures how much time you spent configuring the matching solution to produce this experiment. Select a file containing the output of the matching solution and choose the correct import format . Click on Add . this process can take several minutes depending on the size of the experiment.","title":"Adding an experiment"},{"location":"basic_usage/experiments/#previewing-an-experiment","text":"You can preview the experiment by clicking on the telescope button on the bottom of the dataset card to make sure that the experiment was added successfully.","title":"Previewing an experiment"},{"location":"basic_usage/experiments/#editing-an-experiment","text":"After the initial experiment creation, you can still change some attributes of the dataset. To open the experiment editor, click the leftmost button on the experiment card.","title":"Editing an experiment"},{"location":"basic_usage/experiments/#deleting-an-experiment","text":"To delete an experiment click the rightmost button on the experiment card.","title":"Deleting an experiment"},{"location":"basic_usage/experiments/#import-formats","text":"Info The tool only accepts source files in csv format at the moment - so in case your source file is a Microsoft Excel file, you'll first have to export it to csv! (Additionally, make sure that your file is UTF-8 encoded.) To ease the import process, Snowman supports several file formats out of the box. Those include:","title":"Import formats"},{"location":"basic_usage/experiments/#pilot","text":"This file format was introduced with the initial prototype and is the easiest available format. Result sets can be uploaded as (comma-separated) csv files in UTF-8 encoding. We expect the csv file to have , as split character, \" as quote character and ' as escape character. The importer expects the csv to have the columns p1 and p2 . They should store the ids of the respective dataset tuples. The csv optionally can have a column named prediction which contains a 1 in case the pair was detected as duplicate - 0 otherwise. If this column is not present we assume that all listed tuples have been detected as duplicates (think: we automatically insert a column prediction and fill it with the value 1 everywhere). Following this, more columns may be specified which contain similarity scores (numbers). See the following example: p1,p2,prediction,feat1,feat2,feat3,sum 2,1,1,0.3,0.4,0.4,2 1,2,1,0.3,0.4,0.4,2 4,3,1,0.3,0.4,0.4,2 3,4,1,0.3,0.4,0.4,3 6,5,1,0.3,0.5,0.4,3 5,6,1,0.3,0.5,0.4,3 8,7,1,0.3,0.5,0.4,2 7,8,1,0.3,0.5,0.4,2 10,9,1,0.3,0.5,0.4,2 9,10,1,0.3,0.4,0.4,2 12,11,1,0.3,0.4,0.4,2 11,12,1,0.3,0.4,0.4,8 14,13,1,0.3,0.4,0.4,8 13,14,1,0.3,0.4,0.4,8 16,15,1,0.3,0.4,0.4,8 15,16,1,0.3,0.4,0.4,2 ...","title":"Pilot"},{"location":"basic_usage/experiments/#magellan","text":"The open-source matching solution Magellan is widely used in research. We support its result set file format out of the box. ,_id,ltable_id,rtable_id,id_id_exm,id_id_anm,id_id_lev_dist,id_id_lev_sim,name_name_jac_qgm_3_qgm_3,name_name_cos_dlm_dc0_dlm_dc0,name_name_jac_dlm_dc0_dlm_dc0,name_name_mel,name_name_lev_dist,name_name_lev_sim,name_name_nmw,name_name_sw,addr_addr_jac_qgm_3_qgm_3,addr_addr_cos_dlm_dc0_dlm_dc0,addr_addr_jac_dlm_dc0_dlm_dc0,addr_addr_mel,addr_addr_lev_dist,addr_addr_lev_sim,addr_addr_nmw,addr_addr_sw,city_city_jac_qgm_3_qgm_3,city_city_cos_dlm_dc0_dlm_dc0,city_city_jac_dlm_dc0_dlm_dc0,city_city_mel,city_city_lev_dist,city_city_lev_sim,city_city_nmw,city_city_sw,type_type_jac_qgm_3_qgm_3,type_type_cos_dlm_dc0_dlm_dc0,type_type_jac_dlm_dc0_dlm_dc0,type_type_mel,type_type_lev_dist,type_type_lev_sim,type_type_nmw,type_type_sw,gold,predicted 124,2054,598,283,0,0.47324414715719065,3,0.0,1.0,1.0,1.0,1.0,0.0,1.0,11.0,11.0,0.21739130434782608,0.40824829046386296,0.2222222222222222,0.7647619247436523,30.0,0.25,-16.0,9.0,0.47058823529411764,0.8164965809277259,0.6666666666666666,0.9230769276618958,5.0,0.6153846153846154,3.0,8.0,0.0,0.0,0.0,0.4991452991962433,11.0,0.15384615384615385,-6.0,1.0,1,1 54,768,739,115,0,0.15561569688768606,3,0.0,0.1111111111111111,0.40824829046386296,0.25,0.5292929410934448,12.0,0.19999999999999996,-1.0,4.0,0.14814814814814814,0.2886751345948129,0.16666666666666666,0.5842490792274475,9.0,0.3571428571428571,4.0,4.0,0.47058823529411764,0.8164965809277259,0.6666666666666666,0.9230769276618958,5.0,0.6153846153846154,3.0,8.0,0.0,0.0,0.0,0.0,6.0,0.0,-2.0,1.0,0,0 268,1736,1046,246,0,0.23518164435946465,2,0.5,0.058823529411764705,0.33333333333333337,0.2,0.5309057235717773,15.0,0.11764705882352944,0.0,4.0,0.06896551724137931,0.0,0.0,0.6170329451560974,10.0,0.2857142857142857,3.0,4.0,0.0,0.0,0.0,0.5897436141967773,10.0,0.23076923076923073,2.0,3.0,0.3,0.7071067811865475,0.5,0.875,10.0,0.375,-4.0,6.0,0,0 293,618,888,78,0,0.08783783783783783,2,0.33333333333333337,0.2,0.4999999999999999,0.3333333333333333,0.6702020168304443,9.0,0.4,2.0,5.0,0.10810810810810811,0.22360679774997896,0.125,0.510185182094574,17.0,0.29166666666666663,-2.0,6.0,0.47058823529411764,0.8164965809277259,0.6666666666666666,0.9230769276618958,5.0,0.6153846153846154,3.0,8.0,0.0,0.0,0.0,0.5777778029441833,6.0,0.0,-1.0,1.0,0,0 230,486,866,66,0,0.07621247113163976,1,0.6666666666666667,0.18181818181818182,0.35355339059327373,0.2,0.35333332419395447,18.0,0.28,-8.0,6.0,0.02857142857142857,0.0,0.0,0.4783068895339966,14.0,0.2222222222222222,0.0,2.0,0.0,0.0,0.0,0.4611110985279083,13.0,0.1333333333333333,-5.0,1.0,0.0,0.0,0.0,0.5138888955116272,7.0,0.125,-2.0,2.0,0,0 134,2079,599,284,0,0.4741235392320534,3,0.0,1.0,1.0,1.0,1.0,0.0,1.0,17.0,17.0,1.0,1.0,1.0,1.0,0.0,1.0,14.0,14.0,0.47058823529411764,0.8164965809277259,0.6666666666666666,0.9230769276618958,5.0,0.6153846153846154,3.0,8.0,0.4444444444444444,0.7071067811865475,0.5,0.9142857193946838,6.0,0.5714285714285714,2.0,8.0,1,1 12,391,905,48,0,0.053038674033149213,3,0.0,0.19230769230769232,0.40824829046386296,0.25,0.6499999761581421,9.0,0.4,3.0,6.0,0.023809523809523808,0.0,0.0,0.47883597016334534,18.0,0.1428571428571429,0.0,2.0,0.0,0.0,0.0,0.5416666865348816,9.0,0.25,-1.0,3.0,0.4444444444444444,0.7071067811865475,0.5,0.7321428656578064,6.0,0.5714285714285714,2.0,8.0,0,0 423,1450,758,209,0,0.2757255936675461,3,0.0,0.21875,0.35355339059327373,0.2,0.5888888835906982,12.0,0.4,3.0,7.0,0.07692307692307693,0.0,0.0,0.5629629492759705,11.0,0.2666666666666667,-2.0,4.0,0.0,0.0,0.0,0.5352563858032227,12.0,0.07692307692307687,-4.0,2.0,0.0,0.0,0.0,0.4833333194255829,8.0,0.19999999999999996,0.0,2.0,0,0 272,248,797,33,0,0.04140526976160597,3,0.0,0.20833333333333334,0.40824829046386296,0.25,0.6168830990791321,8.0,0.4285714285714286,3.0,6.0,0.2,0.5,0.3333333333333333,0.7516340017318726,9.0,0.47058823529411764,6.0,7.0,0.0,0.0,0.0,0.0,8.0,0.0,-6.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,8.0,8.0,0,0 ...","title":"Magellan"},{"location":"basic_usage/experiments/#sigmod2021","text":"For more information, see here . left_instance_id,right_instance_id,label http://store.com/42,http://otherstore.net/af82,1 http://store.com/243,http://otherstore.net/cn82,0 ... The label (1=duplicate, 0=non-duplicate) is optional in Snowman. A missing label will consider the pair as duplicates. Therefore, the following list would be interpreted as duplicates: left_instance_id,right_instance_id http://store.com/42,http://otherstore.net/af82 http://store.com/243,http://otherstore.net/cn82 ...","title":"SIGMOD2021"},{"location":"basic_usage/experiments/#proprietary-formats","text":"We support a range of proprietary experiment formats. A list of those can be found here: BHANA BPIES ClusterER","title":"Proprietary Formats"},{"location":"basic_usage/getting_started/","text":"Getting Started At the end of this document you will have a working version of Snowman. Setup Download the latest artifact for you platform from the Github Releases page. Extract the zip and run snowman.exe , snowman.app or snowman according to the platform you are on. The benchmark will now start and ask you whether you want to spin up a local instance or connect to a remote instance. Note: The initial startup of Snowman can take some time. Local usage If you select local usage, a folder within your home directory is created. This folder contains all benchmark data. The location of this folder is platform specific. Platform Folder MacOS ~/Library/Application Support/snowman-wrapper Windows C:\\Users\\<you>\\AppData\\Local\\snowman-wrapper Linux ~/.config/snowman-wrapper If you changed something in your environment, these paths may be different. Rest assured that Snowman will not touch any other folders or files :) Remote usage If you received a link to a colleagues instance, enter the link into the input field. The app will now connect to this URL and log into your colleagues instance. If the remote instance cannot be reached, an error message is shown. As a first debug step, attempt to open the URL within your browser. Upgrading from a previous version To upgrade Snowman to the latest version, first delete the folder with the old version of Snowman. This step is not necessary but should be done because running an old version of Snowman after running the newest version will reset all data . Afterwards, simply follow the steps in Setup again. Necessary migration steps will automatically be applied to the stored data. Next Step You now should have a working version of Snowman. As a next step we suggest for you to get familiar with the key concepts and terminology used throughout Snowman.","title":"Getting Started"},{"location":"basic_usage/getting_started/#getting-started","text":"At the end of this document you will have a working version of Snowman.","title":"Getting Started"},{"location":"basic_usage/getting_started/#setup","text":"Download the latest artifact for you platform from the Github Releases page. Extract the zip and run snowman.exe , snowman.app or snowman according to the platform you are on. The benchmark will now start and ask you whether you want to spin up a local instance or connect to a remote instance. Note: The initial startup of Snowman can take some time.","title":"Setup"},{"location":"basic_usage/getting_started/#local-usage","text":"If you select local usage, a folder within your home directory is created. This folder contains all benchmark data. The location of this folder is platform specific. Platform Folder MacOS ~/Library/Application Support/snowman-wrapper Windows C:\\Users\\<you>\\AppData\\Local\\snowman-wrapper Linux ~/.config/snowman-wrapper If you changed something in your environment, these paths may be different. Rest assured that Snowman will not touch any other folders or files :)","title":"Local usage"},{"location":"basic_usage/getting_started/#remote-usage","text":"If you received a link to a colleagues instance, enter the link into the input field. The app will now connect to this URL and log into your colleagues instance. If the remote instance cannot be reached, an error message is shown. As a first debug step, attempt to open the URL within your browser.","title":"Remote usage"},{"location":"basic_usage/getting_started/#upgrading-from-a-previous-version","text":"To upgrade Snowman to the latest version, first delete the folder with the old version of Snowman. This step is not necessary but should be done because running an old version of Snowman after running the newest version will reset all data . Afterwards, simply follow the steps in Setup again. Necessary migration steps will automatically be applied to the stored data.","title":"Upgrading from a previous version"},{"location":"basic_usage/getting_started/#next-step","text":"You now should have a working version of Snowman. As a next step we suggest for you to get familiar with the key concepts and terminology used throughout Snowman.","title":"Next Step"},{"location":"basic_usage/matching_solutions/","text":"Matching Solutions After reading this document you will know how to add your matching solution to Snowman. Further you will know what you can do with matching solutions in the tool. Adding a matching solution Open the tab Matching Solutions . Click on the + button in the lower left corner of the screen. Specify a short name and optionally a comprehensive description. Optionally, open the other sections to fill in more details about the matching solution. Click on Add Deleting a matching solution If a solution was created by accident or is not required anymore, you can delete it by clicking on the rightmost button of the matching solution card. Be aware that all experiments belonging to this matching solution will automatically be deleted.","title":"Matching solutions"},{"location":"basic_usage/matching_solutions/#matching-solutions","text":"After reading this document you will know how to add your matching solution to Snowman. Further you will know what you can do with matching solutions in the tool.","title":"Matching Solutions"},{"location":"basic_usage/matching_solutions/#adding-a-matching-solution","text":"Open the tab Matching Solutions . Click on the + button in the lower left corner of the screen. Specify a short name and optionally a comprehensive description. Optionally, open the other sections to fill in more details about the matching solution. Click on Add","title":"Adding a matching solution"},{"location":"basic_usage/matching_solutions/#deleting-a-matching-solution","text":"If a solution was created by accident or is not required anymore, you can delete it by clicking on the rightmost button of the matching solution card. Be aware that all experiments belonging to this matching solution will automatically be deleted.","title":"Deleting a matching solution"},{"location":"basic_usage/analyses/binary_metrics_view/","text":"Binary Metrics View Info Requires a gold standard. The Binary Metrics View allows you to explore how good an experiment performed by showing it's predictions and performance metrics. Getting Started Add the benchmark dataset to Snowman Add a gold standard for the dataset to Snowman Add the experiment you want to investigate to Snowman Open the Benchmark Dashboard and select the analysis Binary Metrics View . Select the dataset, the gold standard and your experiment in the configurator Interpreting the Results Metrics are calculated and shown in the top carousel. You can step through all the available metric cards and get an overview over the quality of the experiment. Keep in mind that these metrics are only as reliable as the gold standard is! Some metrics may be unreliable when calculated with a silver standard. In the future our tool will highlight such metrics (see this issue ). Also, you are able to inspect the true positives, false positives, false negatives and true negatives in the table below.","title":"Binary Metrics View"},{"location":"basic_usage/analyses/binary_metrics_view/#binary-metrics-view","text":"Info Requires a gold standard. The Binary Metrics View allows you to explore how good an experiment performed by showing it's predictions and performance metrics.","title":"Binary Metrics View"},{"location":"basic_usage/analyses/binary_metrics_view/#getting-started","text":"Add the benchmark dataset to Snowman Add a gold standard for the dataset to Snowman Add the experiment you want to investigate to Snowman Open the Benchmark Dashboard and select the analysis Binary Metrics View . Select the dataset, the gold standard and your experiment in the configurator","title":"Getting Started"},{"location":"basic_usage/analyses/binary_metrics_view/#interpreting-the-results","text":"Metrics are calculated and shown in the top carousel. You can step through all the available metric cards and get an overview over the quality of the experiment. Keep in mind that these metrics are only as reliable as the gold standard is! Some metrics may be unreliable when calculated with a silver standard. In the future our tool will highlight such metrics (see this issue ). Also, you are able to inspect the true positives, false positives, false negatives and true negatives in the table below.","title":"Interpreting the Results"},{"location":"basic_usage/analyses/kpi_decision_matrix/","text":"KPI Decision Matrix The KPI Decision Matrix provides you with the most important kpis to make an informed buying decision. Getting Started Add all matching solutions you want to compare to Snowman If you have experiments of the matching solution for datasets with a gold standard add them to Snowman Open the Benchmark Dashboard and select the analysis KPI Decision Matrix . Select your matching solutions and experiments in the configurator Interpreting the Results KPI Definition Matching Solution Type The architectural type of the matching solution, such as \"Machine Learning\", \"rule-based\" or \"Active Learning\" Use Case Which use cases does the matching solution cover (e.g., merging from two file, deduplicating one file, ...)? Some matching solutions do not support every use case. General Costs These include the cost to buy a matchingsolution, the cost to host the infrastructure, etc.. Since thesecan vary from payment model to payment model (one-timecosts, monthly costs, pay-per-use), they are aggregated overthe entire product lifecycle to increase comparability. Sincethese can vary from payment model to payment model (one-time costs, monthly costs, pay-per-use), they are aggregatedover the entire product lifecycle to increase comparability. Deployment Type This describes in which way the matching solution has to be deployed: Does a hosted instance exist already? Or is it just locally executable? Hosted instances,for example, often require less maintenance and might be therefore a decision criteria. Installation Effort Embedding a matching solution in the software landscape of a company requires a specific knowledge level and amount of time which varies for each matching solution. The integration effort is split into these kpis. Matching Solution Effort Another task is configuring the matching solution itself. Some matching solutions do not require any specific knowledge (for example, solutions where uploading only a file is sufficient), but other matching solution need detailed configurations. Because of that, we propose to measure the effort to configure a matching solution by knowledge-level and HR-Amount. Domain Effort Matching solutions often require domain knowledge during the configuration phase: A machine learning based approach, for example, needs pre-labeled data to fit the model. For that, a person who knows the domain has to classify given pairs of records, in order to create a trainings dataset. For rule-based approaches domain knowledge is needed, too: Rules have to be created, based on pair-wise similarities. To define effective rules, it requires domain knowledge. This results in a domain effort for the specific matching solution, measured by the required expertise and HR-Amount. Interfaces This describes the provided interfaces to access the matching solution (e.g.: GUI, API, CLI). Depending on the use case it might be necessary that the matching solution provides a specific interface: For example, the automation of specific data matching tasks requires an API or CLI. Supported OS On which operating systems is it possible to execute the data matching solution? Info knowledge-level and hr-Amount are mutually dependent. A person with a higher knowledge-level does usually need less time to configure a specific matching solution. To reflect this issue, we provide an aggregated measure. This is calculated with the manhattan-distance between the knowledge-level and the hr-Amount","title":"KPI Decision Matrix"},{"location":"basic_usage/analyses/kpi_decision_matrix/#kpi-decision-matrix","text":"The KPI Decision Matrix provides you with the most important kpis to make an informed buying decision.","title":"KPI Decision Matrix"},{"location":"basic_usage/analyses/kpi_decision_matrix/#getting-started","text":"Add all matching solutions you want to compare to Snowman If you have experiments of the matching solution for datasets with a gold standard add them to Snowman Open the Benchmark Dashboard and select the analysis KPI Decision Matrix . Select your matching solutions and experiments in the configurator","title":"Getting Started"},{"location":"basic_usage/analyses/kpi_decision_matrix/#interpreting-the-results","text":"KPI Definition Matching Solution Type The architectural type of the matching solution, such as \"Machine Learning\", \"rule-based\" or \"Active Learning\" Use Case Which use cases does the matching solution cover (e.g., merging from two file, deduplicating one file, ...)? Some matching solutions do not support every use case. General Costs These include the cost to buy a matchingsolution, the cost to host the infrastructure, etc.. Since thesecan vary from payment model to payment model (one-timecosts, monthly costs, pay-per-use), they are aggregated overthe entire product lifecycle to increase comparability. Sincethese can vary from payment model to payment model (one-time costs, monthly costs, pay-per-use), they are aggregatedover the entire product lifecycle to increase comparability. Deployment Type This describes in which way the matching solution has to be deployed: Does a hosted instance exist already? Or is it just locally executable? Hosted instances,for example, often require less maintenance and might be therefore a decision criteria. Installation Effort Embedding a matching solution in the software landscape of a company requires a specific knowledge level and amount of time which varies for each matching solution. The integration effort is split into these kpis. Matching Solution Effort Another task is configuring the matching solution itself. Some matching solutions do not require any specific knowledge (for example, solutions where uploading only a file is sufficient), but other matching solution need detailed configurations. Because of that, we propose to measure the effort to configure a matching solution by knowledge-level and HR-Amount. Domain Effort Matching solutions often require domain knowledge during the configuration phase: A machine learning based approach, for example, needs pre-labeled data to fit the model. For that, a person who knows the domain has to classify given pairs of records, in order to create a trainings dataset. For rule-based approaches domain knowledge is needed, too: Rules have to be created, based on pair-wise similarities. To define effective rules, it requires domain knowledge. This results in a domain effort for the specific matching solution, measured by the required expertise and HR-Amount. Interfaces This describes the provided interfaces to access the matching solution (e.g.: GUI, API, CLI). Depending on the use case it might be necessary that the matching solution provides a specific interface: For example, the automation of specific data matching tasks requires an API or CLI. Supported OS On which operating systems is it possible to execute the data matching solution? Info knowledge-level and hr-Amount are mutually dependent. A person with a higher knowledge-level does usually need less time to configure a specific matching solution. To reflect this issue, we provide an aggregated measure. This is calculated with the manhattan-distance between the knowledge-level and the hr-Amount","title":"Interpreting the Results"},{"location":"basic_usage/analyses/kpi_investigator/","text":"KPI Investigator The KPI Investigator allows you to contrast accross matching solutions metrics, such as precision, with soft kpis, such as configuration effort. Getting Started Add all matching solutions you want to compare to Snowman Add at least one experiment for every matching solution and enter the metrics you want to compare add more experiments to improve the evaluation quality Open the Benchmark Dashboard and select the analysis KPI Investigator . Select your experiments in the configurator You can select the metrics you want to compare from the dropdowns X Axis Metric and Y Axis Metric at the top of the page. Optionally choose whether to group (and color) the result by matching solution or dataset with the rightmost dropdown. Interpreting the Results In the screenshot above, the x-axis shows the needed hr amount (in person hours) to install a matching solution. The y-axis shows the recall of specific experiments. If your specific use case requires a recall of 85% or higher, Mock Solution might be better for you as it reaches this precision earlier. If your goal is to achive the highest possible performance without looking at the effort to get there, Magellan might be better as it overtakes Mock Solution after 20 hours of configuration. Many other evaluations are possible. You can for example measure the difficulity of a dataset by adding experiments of different matching solutions for multiple datasets, grouping by dataset, and then viewing two performance metrics like precision/recall.","title":"KPI Investigator"},{"location":"basic_usage/analyses/kpi_investigator/#kpi-investigator","text":"The KPI Investigator allows you to contrast accross matching solutions metrics, such as precision, with soft kpis, such as configuration effort.","title":"KPI Investigator"},{"location":"basic_usage/analyses/kpi_investigator/#getting-started","text":"Add all matching solutions you want to compare to Snowman Add at least one experiment for every matching solution and enter the metrics you want to compare add more experiments to improve the evaluation quality Open the Benchmark Dashboard and select the analysis KPI Investigator . Select your experiments in the configurator You can select the metrics you want to compare from the dropdowns X Axis Metric and Y Axis Metric at the top of the page. Optionally choose whether to group (and color) the result by matching solution or dataset with the rightmost dropdown.","title":"Getting Started"},{"location":"basic_usage/analyses/kpi_investigator/#interpreting-the-results","text":"In the screenshot above, the x-axis shows the needed hr amount (in person hours) to install a matching solution. The y-axis shows the recall of specific experiments. If your specific use case requires a recall of 85% or higher, Mock Solution might be better for you as it reaches this precision earlier. If your goal is to achive the highest possible performance without looking at the effort to get there, Magellan might be better as it overtakes Mock Solution after 20 hours of configuration. Many other evaluations are possible. You can for example measure the difficulity of a dataset by adding experiments of different matching solutions for multiple datasets, grouping by dataset, and then viewing two performance metrics like precision/recall.","title":"Interpreting the Results"},{"location":"basic_usage/analyses/n_intersection_viewer/","text":"N-Intersection Viewer The N-Intersection Viewer allows you to investigate which duplicates have (not) been detected by which matching solution. Getting Started Add the benchmark dataset to Snowman Optionally add a gold standard for the dataset to Snowman Add the experiments you want to investigate to Snowman Open the Benchmark Dashboard and select the analysis N-Intersection Viewer . Select the dataset, optionally the gold standard and your experiments in the configurator Interpreting the Results The N-Intersection viewer can for example answer the question which ground truth duplicate pairs have not been found by one or more experiments (a gold standard is not required for this page). In the screenshot above all pairs are displayed which belong to the goldstandard but not to hpi-run2. You could now investigate why hpi-run2 does not classify those pairs as duplicates and improve the matching solution accordingly. Clicking on an area in the Venn diagram intersects all experiments which are present in this area. By using the drag'n'drop selector below the Venn diagram, other experiments can now be excluded from the intersection. The example above could therefore be achieved by first clicking on the green area in the picture and then dragging hpi-run2 from available to exclude .","title":"N-Intersection Viewer"},{"location":"basic_usage/analyses/n_intersection_viewer/#n-intersection-viewer","text":"The N-Intersection Viewer allows you to investigate which duplicates have (not) been detected by which matching solution.","title":"N-Intersection Viewer"},{"location":"basic_usage/analyses/n_intersection_viewer/#getting-started","text":"Add the benchmark dataset to Snowman Optionally add a gold standard for the dataset to Snowman Add the experiments you want to investigate to Snowman Open the Benchmark Dashboard and select the analysis N-Intersection Viewer . Select the dataset, optionally the gold standard and your experiments in the configurator","title":"Getting Started"},{"location":"basic_usage/analyses/n_intersection_viewer/#interpreting-the-results","text":"The N-Intersection viewer can for example answer the question which ground truth duplicate pairs have not been found by one or more experiments (a gold standard is not required for this page). In the screenshot above all pairs are displayed which belong to the goldstandard but not to hpi-run2. You could now investigate why hpi-run2 does not classify those pairs as duplicates and improve the matching solution accordingly. Clicking on an area in the Venn diagram intersects all experiments which are present in this area. By using the drag'n'drop selector below the Venn diagram, other experiments can now be excluded from the intersection. The example above could therefore be achieved by first clicking on the green area in the picture and then dragging hpi-run2 from available to exclude .","title":"Interpreting the Results"},{"location":"basic_usage/analyses/n_metrics_table/","text":"N-Metrics Table Info Requires a gold standard. The N-Metrics Table allows you to compare performance metrics of multiple experiments. Getting Started Add the benchmark dataset to Snowman Add a gold standard for the dataset to Snowman Add the experiments you want to investigate to Snowman Open the Benchmark Dashboard and select the analysis N-Metrics Table . Select the dataset, the gold standard and your experiments in the configurator Interpreting the Results Whenever you want to compare multiple experiments against a single ground truth, you can make use of the N-Metrics Table. It extends the Binary Metrics View across multiple experiments and presents the result in a table format. Clicking on an experiment in the header opens the Binary Metrics View for this experiment. Hover over a metric or a value to see more information.","title":"N-Metrics Table"},{"location":"basic_usage/analyses/n_metrics_table/#n-metrics-table","text":"Info Requires a gold standard. The N-Metrics Table allows you to compare performance metrics of multiple experiments.","title":"N-Metrics Table"},{"location":"basic_usage/analyses/n_metrics_table/#getting-started","text":"Add the benchmark dataset to Snowman Add a gold standard for the dataset to Snowman Add the experiments you want to investigate to Snowman Open the Benchmark Dashboard and select the analysis N-Metrics Table . Select the dataset, the gold standard and your experiments in the configurator","title":"Getting Started"},{"location":"basic_usage/analyses/n_metrics_table/#interpreting-the-results","text":"Whenever you want to compare multiple experiments against a single ground truth, you can make use of the N-Metrics Table. It extends the Binary Metrics View across multiple experiments and presents the result in a table format. Clicking on an experiment in the header opens the Binary Metrics View for this experiment. Hover over a metric or a value to see more information.","title":"Interpreting the Results"},{"location":"basic_usage/analyses/overview/","text":"Overview Analysis Description Screenshot Binary Metrics View The Binary Metrics View allows you to explore how good an experiment performed by showing it's predictions and performance metrics. KPI Decision Matrix The KPI Decision Matrix provides you with the most important kpis to make an informed buying decision. KPI Investigator The KPI Investigator allows you to contrast accross matching solutions metrics, such as precision, with soft kpis, such as configuration effort. N-Intersection Viewer The N-Intersection Viewer allows you to investigate which duplicates have (not) been detected by which matching solution. N-Metrics Table The N-Metrics Table allows you to compare performance metrics of multiple experiments. Similarity Diagrams The Similarity Diagrams analysis allows you to find the best similarity threshold.","title":"Overview"},{"location":"basic_usage/analyses/overview/#overview","text":"Analysis Description Screenshot Binary Metrics View The Binary Metrics View allows you to explore how good an experiment performed by showing it's predictions and performance metrics. KPI Decision Matrix The KPI Decision Matrix provides you with the most important kpis to make an informed buying decision. KPI Investigator The KPI Investigator allows you to contrast accross matching solutions metrics, such as precision, with soft kpis, such as configuration effort. N-Intersection Viewer The N-Intersection Viewer allows you to investigate which duplicates have (not) been detected by which matching solution. N-Metrics Table The N-Metrics Table allows you to compare performance metrics of multiple experiments. Similarity Diagrams The Similarity Diagrams analysis allows you to find the best similarity threshold.","title":"Overview"},{"location":"basic_usage/analyses/similarity_diagrams/","text":"Similarity Diagrams Info Requires a gold standard. Info Your matching solution must output a similarity score next to the matching decision. The Similarity Diagrams analysis allows you to find the best similarity threshold. Getting Started Add the benchmark datasets to Snowman Add gold standards for the datasets to Snowman Add the experiments you want to investigate to Snowman Make sure that the experiment files you select contain a column for every similarity score you want to investigate. Some matching solutions like Magellan automatically export similarity scores. If Snowman supports the experiment format, the similarity scores will automatically be detected. Open the Benchmark Dashboard and select the analysis Similarity Diagrams . Select the datasets, gold standards and experiments in the configurator Select a similarity score to be investigated for every experiment Interpreting the Results This analysis allows you to see how metrics, such as precision or recall, evolve with different similarity thresholds. Therefore, it helps you to find the best threshold for your use case. You can hover over points in the diagram to show the similarity threshold they correspond to. The metrics on the x axis and y axis can be changed with the dropdowns at the top of the page.","title":"Similarity Diagrams"},{"location":"basic_usage/analyses/similarity_diagrams/#similarity-diagrams","text":"Info Requires a gold standard. Info Your matching solution must output a similarity score next to the matching decision. The Similarity Diagrams analysis allows you to find the best similarity threshold.","title":"Similarity Diagrams"},{"location":"basic_usage/analyses/similarity_diagrams/#getting-started","text":"Add the benchmark datasets to Snowman Add gold standards for the datasets to Snowman Add the experiments you want to investigate to Snowman Make sure that the experiment files you select contain a column for every similarity score you want to investigate. Some matching solutions like Magellan automatically export similarity scores. If Snowman supports the experiment format, the similarity scores will automatically be detected. Open the Benchmark Dashboard and select the analysis Similarity Diagrams . Select the datasets, gold standards and experiments in the configurator Select a similarity score to be investigated for every experiment","title":"Getting Started"},{"location":"basic_usage/analyses/similarity_diagrams/#interpreting-the-results","text":"This analysis allows you to see how metrics, such as precision or recall, evolve with different similarity thresholds. Therefore, it helps you to find the best threshold for your use case. You can hover over points in the diagram to show the similarity threshold they correspond to. The metrics on the x axis and y axis can be changed with the dropdowns at the top of the page.","title":"Interpreting the Results"},{"location":"basic_usage/workflows/comparing_matching_solutions/","text":"Comparing Matching Solutions Comparing matching solutions and selecting the best one for a specific use case is a difficult task. Many different factors influence the decision. Mainly they can be grouped by the following: The general performance of a matching solution, measured with metrics, such as precision or recall The effort to configure a matching solution , measured with metrics, such as the time to configure or total cost of ownership Snowman provides two different analysis tools designed for specifically this use case: Analysis Description Screenshot KPI Decision Matrix The KPI Decision Matrix provides you with the most important kpis to make an informed buying decision. KPI Investigator The KPI Investigator allows you to contrast accross matching solutions metrics, such as precision, with soft kpis, such as configuration effort.","title":"Comparing Matching Solutions"},{"location":"basic_usage/workflows/comparing_matching_solutions/#comparing-matching-solutions","text":"Comparing matching solutions and selecting the best one for a specific use case is a difficult task. Many different factors influence the decision. Mainly they can be grouped by the following: The general performance of a matching solution, measured with metrics, such as precision or recall The effort to configure a matching solution , measured with metrics, such as the time to configure or total cost of ownership Snowman provides two different analysis tools designed for specifically this use case: Analysis Description Screenshot KPI Decision Matrix The KPI Decision Matrix provides you with the most important kpis to make an informed buying decision. KPI Investigator The KPI Investigator allows you to contrast accross matching solutions metrics, such as precision, with soft kpis, such as configuration effort.","title":"Comparing Matching Solutions"},{"location":"basic_usage/workflows/improving_matching_solutions/","text":"Improving Matching Solutions Snowman provides multiple analysis tools which aim to help developers improving their matching solutions. They can also be used to ease the configuration process necessary to set up a matching solution for a specific use case. Keep in mind that most of these analyses require a gold standard. Analysis Description Screenshot Binary Metrics View The Binary Metrics View allows you to explore how good an experiment performed by showing it's predictions and performance metrics. N-Intersection Viewer The N-Intersection Viewer allows you to investigate which duplicates have (not) been detected by which matching solution. N-Metrics Table The N-Metrics Table allows you to compare performance metrics of multiple experiments. Similarity Diagrams The Similarity Diagrams analysis allows you to find the best similarity threshold.","title":"Improving Matching Solutions"},{"location":"basic_usage/workflows/improving_matching_solutions/#improving-matching-solutions","text":"Snowman provides multiple analysis tools which aim to help developers improving their matching solutions. They can also be used to ease the configuration process necessary to set up a matching solution for a specific use case. Keep in mind that most of these analyses require a gold standard. Analysis Description Screenshot Binary Metrics View The Binary Metrics View allows you to explore how good an experiment performed by showing it's predictions and performance metrics. N-Intersection Viewer The N-Intersection Viewer allows you to investigate which duplicates have (not) been detected by which matching solution. N-Metrics Table The N-Metrics Table allows you to compare performance metrics of multiple experiments. Similarity Diagrams The Similarity Diagrams analysis allows you to find the best similarity threshold.","title":"Improving Matching Solutions"},{"location":"dev_setup/contrib-knowledge/","text":"Contribution Knowledge This document contains information about how to extend the codebase in specific cases. For general contributing guidelines see here . Adding new experiment formats subclass ExperimentInserter and overwrite the abstract methods add duplicates via addDuplicate we also provide a CSVInserter which can be subclassed for csv experiment formats give the format a unique name and register it with the server by adding it to this map. add the format to the api specification ( under docs ) regenerate the types and generated api specification Updating the database schema change the schema under api/database/schemas as required create a migration script in api/database/schemas/migrations which does not reference the schema (as it might change in the future) that means the changes should be \"duplicated\" here. See api/database/schemas/v3.ts as an example. You can overwrite performResetAsMigration if a migration is not possible and the database must be recreated. update api/database/schemas/migrations/index.ts and export the new version as latest Creating a release update version numbers in ./package.json , ./app/package.json and ./wrapper/package.json to the new target version. merge all changes in to the main branch with a PR \"Release vX.x.x\". tag the latest commit on main in git with vX.x.x according to the version number. wait for the CI to finish releasing the tag. edit the release draft (Github Release): add a changelog and change the artifact names accordingly.","title":"Contributing"},{"location":"dev_setup/contrib-knowledge/#contribution-knowledge","text":"This document contains information about how to extend the codebase in specific cases. For general contributing guidelines see here .","title":"Contribution Knowledge"},{"location":"dev_setup/contrib-knowledge/#adding-new-experiment-formats","text":"subclass ExperimentInserter and overwrite the abstract methods add duplicates via addDuplicate we also provide a CSVInserter which can be subclassed for csv experiment formats give the format a unique name and register it with the server by adding it to this map. add the format to the api specification ( under docs ) regenerate the types and generated api specification","title":"Adding new experiment formats"},{"location":"dev_setup/contrib-knowledge/#updating-the-database-schema","text":"change the schema under api/database/schemas as required create a migration script in api/database/schemas/migrations which does not reference the schema (as it might change in the future) that means the changes should be \"duplicated\" here. See api/database/schemas/v3.ts as an example. You can overwrite performResetAsMigration if a migration is not possible and the database must be recreated. update api/database/schemas/migrations/index.ts and export the new version as latest","title":"Updating the database schema"},{"location":"dev_setup/contrib-knowledge/#creating-a-release","text":"update version numbers in ./package.json , ./app/package.json and ./wrapper/package.json to the new target version. merge all changes in to the main branch with a PR \"Release vX.x.x\". tag the latest commit on main in git with vX.x.x according to the version number. wait for the CI to finish releasing the tag. edit the release draft (Github Release): add a changelog and change the artifact names accordingly.","title":"Creating a release"},{"location":"dev_setup/introduction/","text":"Development setup Folder structure The benchmark is split into three separate packages: ./ : integration tests and linters for Typescript and CSS ./app : our React frontend ./wrapper : our backend + electron to wrap everything up and build a single binary Installation install NodeJS version 14.x or later install a C++ compiler and add it to your PATH the easiest way to accomplish this on Windows is installing the VisualStudio BuildTools for C++ (+ reboot) install Python3 for your distribution run npm install in ./ , ./app and ./wrapper Python3 and C++ are required to build the native extensions for sqlite3. Running Manual run npm start-api in ./wrapper to start the backend run npm start in ./app to start the frontend and open a browser If you want to start the electron wrapper: run npm run release-app in ./ to build and copy the app to ./wrapper run npm run erebuild in ./wrapper to recompile the dependencies to the NodeJS version of electron run npm run start in ./wrapper to start backend and frontend inside electron run npm run rebuild in ./wrapper to recompile the dependencies back to the global NodeJS version VSCode Run the Start Stack compound to start frontend and backend and open Chrome. You can now use breakpoints in ./app and ./wrapper/api . Building run npm run release in ./ to build the frontend and backend and package them in an executable file the executable will be located at ./wrapper/build/<platform>/ Testing run npm run lint or npm run lint-fix in ./ to lint the project run npm run test in ./ (integration tests), ./app (frontend tests) and ./wrapper (backend tests) to test the project the integration tests require a running instance of the backend and the frontend use npm run test-ci to automatically start frontend and backend before running the integration tests Backend API The backend API can also be reached directly. Have a look at our REST API specification for details. Command Line Arguments The backend supports the following command line arguments: --storageDirectory path/to/directory : Where the database and configuration files live default if starting via npm run start-api : .../wrapper/storage default if starting via npm run start or executable: electrons userData folder --inMemory : If present, an in memory database will be used. This increases the performance. Keep in mind that any changes made will be lost when the process is stopped. default: not present --hostname example.org : The API will be available on this hostname only. default: localhost --port 12345 : The API will be available on this port. default: 8123 --headless : If present, does not show the UI but starts the API directly. default: not present --limitMemory 100000000 : If present, limit the amount of memory (RAM). The allocated memory grows linear with this number. Incrementing the amount by one will increase the maximum amount of memory by a few bytes to a few hundred bytes. default: not present Those arguments can be passed to npm run start and npm run start-api in ./wrapper ( do not forget to use -- to separate the npm command and the arguments ) the built executable file","title":"Setup"},{"location":"dev_setup/introduction/#development-setup","text":"","title":"Development setup"},{"location":"dev_setup/introduction/#folder-structure","text":"The benchmark is split into three separate packages: ./ : integration tests and linters for Typescript and CSS ./app : our React frontend ./wrapper : our backend + electron to wrap everything up and build a single binary","title":"Folder structure"},{"location":"dev_setup/introduction/#installation","text":"install NodeJS version 14.x or later install a C++ compiler and add it to your PATH the easiest way to accomplish this on Windows is installing the VisualStudio BuildTools for C++ (+ reboot) install Python3 for your distribution run npm install in ./ , ./app and ./wrapper Python3 and C++ are required to build the native extensions for sqlite3.","title":"Installation"},{"location":"dev_setup/introduction/#running","text":"","title":"Running"},{"location":"dev_setup/introduction/#manual","text":"run npm start-api in ./wrapper to start the backend run npm start in ./app to start the frontend and open a browser If you want to start the electron wrapper: run npm run release-app in ./ to build and copy the app to ./wrapper run npm run erebuild in ./wrapper to recompile the dependencies to the NodeJS version of electron run npm run start in ./wrapper to start backend and frontend inside electron run npm run rebuild in ./wrapper to recompile the dependencies back to the global NodeJS version","title":"Manual"},{"location":"dev_setup/introduction/#vscode","text":"Run the Start Stack compound to start frontend and backend and open Chrome. You can now use breakpoints in ./app and ./wrapper/api .","title":"VSCode"},{"location":"dev_setup/introduction/#building","text":"run npm run release in ./ to build the frontend and backend and package them in an executable file the executable will be located at ./wrapper/build/<platform>/","title":"Building"},{"location":"dev_setup/introduction/#testing","text":"run npm run lint or npm run lint-fix in ./ to lint the project run npm run test in ./ (integration tests), ./app (frontend tests) and ./wrapper (backend tests) to test the project the integration tests require a running instance of the backend and the frontend use npm run test-ci to automatically start frontend and backend before running the integration tests","title":"Testing"},{"location":"dev_setup/introduction/#backend-api","text":"The backend API can also be reached directly. Have a look at our REST API specification for details.","title":"Backend API"},{"location":"dev_setup/introduction/#command-line-arguments","text":"The backend supports the following command line arguments: --storageDirectory path/to/directory : Where the database and configuration files live default if starting via npm run start-api : .../wrapper/storage default if starting via npm run start or executable: electrons userData folder --inMemory : If present, an in memory database will be used. This increases the performance. Keep in mind that any changes made will be lost when the process is stopped. default: not present --hostname example.org : The API will be available on this hostname only. default: localhost --port 12345 : The API will be available on this port. default: 8123 --headless : If present, does not show the UI but starts the API directly. default: not present --limitMemory 100000000 : If present, limit the amount of memory (RAM). The allocated memory grows linear with this number. Incrementing the amount by one will increase the maximum amount of memory by a few bytes to a few hundred bytes. default: not present Those arguments can be passed to npm run start and npm run start-api in ./wrapper ( do not forget to use -- to separate the npm command and the arguments ) the built executable file","title":"Command Line Arguments"}]}